{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOS MNIST Example\n",
    "\n",
    "## Outline\n",
    "* [**Introduction**](#Introduction)\n",
    "* [**Import Modules**](#Import-modules)\n",
    "* [**Setup PyTorch**](#Setup-PyTorch)\n",
    "* [**Setup FOS**](#Setup-FOS)\n",
    "* [**Run the Training**](#Run-the-training)\n",
    "* [**Vizualization**](#Visualization)\n",
    "* [**Save and Restore**](#Save-and-restore-training-sessions)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates how to use **FOS** to train a MNIST model. The input for this model are scans of handwritten digits (0-9) and the target is the digit that it represents. There is only one digit per image and all images have the same shape. The dataset required to train MNIST is part of `Torchvision` and will be downloaded the first time you run this notebook.\n",
    "\n",
    "## Import modules\n",
    "Besides the Supervisor and Trainer we now also import three meters we are going to use to display metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Import the Fos classes we are going to use in this notebook\n",
    "from fos import Workout\n",
    "from fos.callbacks import NotebookMeter\n",
    "from fos.metrics import SingleClassAccuracy, plot_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "\n",
    "The first part of the script is setting up the data loaders and the model architecture. There are no specific FOS classes required here. FOS will work just fine with regular PyTorch classes like the `DataLoader` and the `nn.Module`. The model isself is a small network with 2 Convolutional layers and 2 Linear layers. Since it is so few layers and parameters, it wil run fine on most CPU's.\n",
    "\n",
    "Also a custom metric like `accuracy` can be easily defined. It is a plain Python function of which the first argument is the predicted value (y) and the second arument is the target value (t). The output of the function is the calculated metric value itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders():\n",
    "    '''Loads the images required to train the model. First time will download them from the Internet.'''\n",
    "    data_transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "    \n",
    "    train_loader = DataLoader(MNIST(download=True, root=\".\", transform=data_transform, train=True),\n",
    "                              batch_size=128, shuffle=True)\n",
    "    \n",
    "    val_loader = DataLoader(MNIST(download=False, root=\".\", transform=data_transform, train=False),\n",
    "                            batch_size=128, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "class DigitPredictor(nn.Module):\n",
    "    '''A simple convolutional network for predicting the digits'''\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                        nn.Conv2d(1, 5, kernel_size=5),\n",
    "                        nn.MaxPool2d(2),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(5, 10, kernel_size=5),\n",
    "                        nn.MaxPool2d(2),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(160, 32),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(32, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup PyTorch\n",
    "\n",
    "We now get to the part where we instantiate everything. The first part is the same as any other PyTorch program. The model (predictor), optimizer and loss functions are all plain PyTorch objects with no FOS magic going on. The same holds true for the two dataloaders. If you are not yet familiar with this, there are some excellent tutorials on the [PyTorch](https://pytorch.org) website.\n",
    "\n",
    "If you have a GPU, you can change the code below that instantiates the predictor to `DigitPredictor().to(\"cuda\")` in order to speedup the training. However since this is a small model it will also perform fine on most modern CPU's.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model   = DigitPredictor()\n",
    "workout = Workout(model, F.nll_loss, acc=SingleClassAccuracy())\n",
    "\n",
    "data_train, data_valid = get_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup FOS\n",
    "\n",
    "The following section is where we setup the *FOS* part of the program. We start with creating some meters to capture the metrics. In this example we want to use two meters:\n",
    "\n",
    "* `NotebookMeter` that will print the metrics in the Notebook.\n",
    "* `MemoryMeter` that will just store the captured metrics in memory. \n",
    "  We will later use this to visualize the accuracy during training and validation\n",
    "\n",
    "The `MultiMeter` is then created that includes these two meters since the trainer only accepts a single Meter object as a parameter. When the trainer forwards metrics to the MultiMeter, the MultiMeter will delegate this to these two meters. This way any arbritrary number of meters can be included in the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create a Supervisor you need to always provide the predictor (the model you want to train) and the loss function. Optionally you can pass extra metrics functions to the Supervisor initializer as a dictionary. \n",
    "\n",
    "The `key` is the name that will be used to publish the metric and the `value` is the actual metric function itself. For training phase the metric will be published under the name provided. And for the validation phase it will be prepended with `val_` in order to be able to distinguish the two. So in this case there will be `acc` and `val_acc` metrics displayed:\n",
    "\n",
    "     [ 10:  5159] loss=0.04010 acc=0.98773 val_loss=0.04031 val_acc=0.98625 : 100%|██████████|00:15<00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[  1:   469] loss=2.25619 acc=0.27083 val_loss=2.26755 val_acc=0.20439 : 100%|██████████|00:09<00:00\n[  2:   938] loss=2.20303 acc=0.30208 val_loss=2.18733 val_acc=0.36201 : 100%|██████████|00:10<00:00\n[  3:  1407] loss=1.97001 acc=0.63542 val_loss=1.95138 val_acc=0.60067 : 100%|██████████|00:10<00:00\n[  4:  1876] loss=1.30925 acc=0.71875 val_loss=1.27189 val_acc=0.72328 : 100%|██████████|00:10<00:00\n[  5:  2345] loss=0.80655 acc=0.82292 val_loss=0.67000 val_acc=0.82969 : 100%|██████████|00:09<00:00"
    }
   ],
   "source": [
    "meter = NotebookMeter([\"loss\", \"acc\", \"val_loss\", \"val_acc\"])\n",
    "workout.fit(data_train, data_valid, epochs=5, callbacks=meter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training\n",
    "\n",
    "Remains running the trainer for a number of epochs. When we run the trainer, we supply both the data for training and validation phases and specify for how many epochs we want to run this training. If the validation data is not provided, the validation phase is skipped all together. And if no value for the numper of epochs is provided, 1 epoch is assumed.\n",
    "\n",
    "You should see a validation accuracy of above 98% after the first 5 epochs. Since the trainer keeps track of the state, you can rerun the cell multiple times and it just continues where it left off (the epoch and step counters don't reset). This allows for a iterative development approach in for example a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "The memory meter captured the metrics and can now be used to access the metrics. In this case we plot some graphs. We use the `get_history` method and this returns both the steps (X axis) and values (Y axis) of a particular metric. So even metrics that are captured at different steps can be plotted in the same graph. \n",
    "\n",
    "The result in this case will be passed as an argument to `plot` method of pyplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plot_metrics(plt, workout, [\"loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and restore training sessions\n",
    "\n",
    "If you want to save the trainer and its state, use the `trainer.save()` method. You can optionally specify a file name or let otherwise a directory and filename be generated for you. The trainer will save the following state:\n",
    "\n",
    "1. The Supervisor state (including the underlying predictor model state)\n",
    "2. The Trainer state (including the optimizer state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workout.save(\"my_best_model.pty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again if at later stage you want to continue where you left off or perhaps revert back to a previously saved version, just load the state from one of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workout.load(\"my_best_model.pty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}