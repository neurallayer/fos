{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This sample notebook shows the minimum required steps to train a model using **Fos**. Fos (the Greek word for light) is a Python framework that assists in the development of state of the art machine learning models in PyTorch. The primary focus is on making it a repeatable process with an easy to use API and good support for getting the nessecary insights into your model.  \n",
    "\n",
    "This notebook trains a convolutional neural network that comes out of the box with torchvision, namely `resnet18`. The model takes as input an image of 224x224 pixels and predicts which of the possible 1000 object types are present in the image. To find our mote about the network architecture, checkout this [paper](https://arxiv.org/pdf/1512.03385.pdf). \n",
    "\n",
    "Since the main purpose is to demonstrate the use of Fos, we don't bother to download the pretrained weights or real images and save some bandwith. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "*Fos* supports the standard PyTorch classes and functions for defining models, dataloaders, loss functions and optimizers. So the first few  import statements should look very familiar if you have worked with PyTorch before.\n",
    "\n",
    "For this notebook there are 3 classes required that belong to the *Fos* module: `SuperModel`, `NotebookMeter` and `Trainer`. Their purpose is explained later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18 \n",
    "\n",
    "# Import theFos classes we'll use\n",
    "from fos import SuperModel, Trainer\n",
    "from fos.meters import NotebookMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We first create an instance of the model we want to train, so in this case the resnet18 model. Throughout this example we refer to the instance of model as the `predictor` in order to be able to differentiate from the supermodel we create later on.\n",
    "\n",
    "After the predictor is instantiated, the optimizer and loss function are created. If you are familiar with PyTorch this should all be straight forward. We choose Adam as the optimizer since it performs well in many scenarios. But feel free to swap it out for any other optimizer. And as a loss function we choose the `binary cross entropy` as that fits the multi-class classification problem well.\n",
    "\n",
    "And finally time to create some random dummy data that mimics an image of 224x224 pixels and the target: \n",
    "\n",
    "     X: 4x3x224x224 = 4 samples in a batch x 3 channels (RGB) x 224 pixels width x 224 pixels height\n",
    "     \n",
    "     Y: 4x1000      = 4 samples in a batch x 1000 possible classes \n",
    "\n",
    "In a real world scenario's this would typically be implemented as a PyTorch Dataloader. But for the purpose of this notebook a simple list of random tensors will do just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = resnet18()\n",
    "optim     = torch.optim.Adam(predictor.parameters())\n",
    "loss      = F.binary_cross_entropy_with_logits\n",
    "\n",
    "data = [(torch.randn(4,3,224,224), torch.rand(4,1000).round()) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready to create the three nessecary **Fos**  objects:\n",
    "\n",
    "1. A **SuperModel** (short for SupervisedModel) that adds a loss function to the predictor that you want to train. The SuperModel      is used by the trainer to perform the actual updating of the model. Throughout the examples we call this instance `model`. Under the hood, the SuperModel is still inheriting from `nn.Module`, so can be used were you would also any other model. \n",
    "\n",
    "2. A **Meter** that handles the generated metrics like loss and custom metrics, although in this example there are no custom metrics defined. Here we use the NotebookMeter that will print the following info in a Jupyter notebook:\n",
    "    * the global epoch and step counter\n",
    "    * the metrics (in this case just the training loss)\n",
    "    * the progress (percentage and time) per epoch\n",
    "              \n",
    "              \n",
    "3. The **Trainer** that glues everything together and performs the training. When creating the trainer object, you need to pass as arguments a supermodel, an optimizer and a meter.\n",
    "\n",
    "And that is really all that is required. So the minimum is three lines of **Fos** specific code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model   = SuperModel(predictor, loss)\n",
    "meter   = NotebookMeter()\n",
    "trainer = Trainer(model, optim, meter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "With everything being setup, the training can now start. In this case we run the training for 5 epochs. The trainer keeps track of the state of the training, so you can rerun the cell below multiple times and it will continue where it left off.\n",
    "\n",
    "We use random data, so you can safely ignore the loss values that are being printed. But just in case you are curious, since we only have a small set of training data the loss will go down quickly (clearly a case of overfitting). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[  0:    10] loss=0.73133 val_loss=0.69004 : 100%|██████████|00:05<00:00\n",
      "[  1:    20] loss=0.66499 val_loss=0.67383 : 100%|██████████|00:05<00:00\n",
      "[  2:    30] loss=0.64372 val_loss=0.62414 : 100%|██████████|00:05<00:00\n",
      "[  3:    40] loss=0.60462 val_loss=0.56690 : 100%|██████████|00:05<00:00\n",
      "[  4:    50] loss=0.57321 val_loss=0.53663 : 100%|██████████|00:05<00:00\n"
     ]
    }
   ],
   "source": [
    "trainer.run(data, data, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We now have a trained model and that is all for this notebook. But there are other notebooks available that dive into much more details and there is the API documentation itself. \n",
    "\n",
    "Please visit [innerlogic.ai](innerlogic.ai) to find out more.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
