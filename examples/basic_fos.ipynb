{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOS Basic Example\n",
    "\n",
    "## Outline\n",
    "* [**Introduction**](#Introduction)\n",
    "* [**Import Modules**](#Import-modules)\n",
    "* [**Setup**](#Setup)\n",
    "* [**Train**](#Train)\n",
    "* [**Next Steps**](#Next-Steps)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This sample notebook shows the minimum required steps to train a model using **FOS**. FOS (the Greek word for light) is a Python framework that assists in the development of state of the art machine learning models in PyTorch. The primary focus is on making it a repeatable process with an easy to use API and good support for getting the nessecary insights into your model.  \n",
    "\n",
    "This notebook trains a convolutional neural network that comes out of the box with torchvision, namely `resnet18`. The model takes as input an image of 224x224 pixels and predicts which of the possible 1000 object types are present in the image. To find our mote about the network architecture, checkout this [paper](https://arxiv.org/pdf/1512.03385.pdf). \n",
    "\n",
    "Since the main purpose is to demonstrate the use of FOS, we don't bother to download the pretrained weights or real images and save some bandwidth. \n",
    "\n",
    "## Import modules\n",
    "*FOS* supports the standard PyTorch classes and functions for defining models, dataloaders, loss functions and optimizers. So the first few  import statements should look very familiar if you have worked with PyTorch before.\n",
    "\n",
    "For this notebook there are 3 classes required that belong to the *Fos* module: `Supervisor`, `NotebookMeter` and `Trainer`. Their purpose is explained later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if running on Google Colab\n",
    "# !pip install fos\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18 \n",
    "\n",
    "# Import the FOS classes we'll use in this example\n",
    "from fos import Workout\n",
    "from fos.callbacks import NotebookMeter\n",
    "from fos.metrics import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We first create an instance of the model we want to train, so in this case the resnet18 model. Throughout this example we refer to the instance of model as the `predictor` in order to be able to differentiate from the `Supervisor` we create later on.\n",
    "\n",
    "After the predictor is instantiated, the optimizer and loss function are created. If you are familiar with PyTorch this should all be straight forward. We choose Adam as the optimizer since it performs well in many scenarios. But feel free to swap it out for any other optimizer. And as a loss function we choose the `binary cross entropy` as that fits the multi-class classification problem well.\n",
    "\n",
    "And finally time to create some random dummy data that mimics an image of 224x224 pixels and the target: \n",
    "\n",
    "     X: 4x3x224x224 = 4 samples in a batch x 3 channels (RGB) x 224 pixels width x 224 pixels height\n",
    "     \n",
    "     Y: 4x1000      = 4 samples in a batch x 1000 possible classes \n",
    "\n",
    "In a real world scenario's this would typically be implemented as a PyTorch Dataloader. But for the purpose of this notebook a simple list of random tensors will do just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18().to(\"cuda\")\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss  = F.binary_cross_entropy_with_logits\n",
    "\n",
    "data = [(torch.randn(4,3,224,224), torch.rand(4,1000).round()) for i in range(15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready to create the three nessecary **FOS**  objects:\n",
    "\n",
    "1. A **Supervisor** that adds a loss function to the predictor that you want to train. The Supervisor instance is used by the trainer to perform the actual updating of the model. Throughout the examples we call this instance `model`. Under the hood, the Supervisor is still inheriting from `nn.Module`, so can be used were you would also any other PyTorch model. \n",
    "\n",
    "2. A **Meter** that handles the generated metrics like loss and custom metrics, although in this example there are no custom metrics defined. Here we use the NotebookMeter that will print the following info in a Jupyter notebook:\n",
    "    * the global epoch and step counter\n",
    "    * the metrics (in this case just the training loss)\n",
    "    * the progress (percentage and time) per epoch\n",
    "              \n",
    "              \n",
    "3. The **Trainer** that glues everything together and performs the training. When creating the trainer object, you need to pass as arguments a `supervisor`, an `optimizer` and a `meter`.\n",
    "\n",
    "And that is really all that is required. So the minimum is three lines of **FOS** specific code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workout = Workout(model, loss, optim, acc=BinaryAccuracy())\n",
    "meter   = NotebookMeter([\"loss\", \"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "With everything being setup, the training can now start. In this case we run the training for 5 epochs. The trainer keeps track of the state of the training, so you can rerun the cell below multiple times and it will continue where it left off. This is ideal when you are working interactively in Notebooks.\n",
    "\n",
    "We use random data, so you can safely ignore the loss values that are being printed. But just in case you are curious, since we only have a small set of training data the loss will go down quickly (clearly a case of overfitting). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[  1:    15] - loss: 0.7199 - acc: 0.5035: 100%|██████████|00:00<00:00\n",
      "[  2:    30] - loss: 0.6739 - acc: 0.5675: 100%|██████████|00:00<00:00\n",
      "[  3:    45] - loss: 0.6786 - acc: 0.5695: 100%|██████████|00:00<00:00\n",
      "[  4:    60] - loss: 0.6698 - acc: 0.5933: 100%|██████████|00:00<00:00\n",
      "[  5:    75] - loss: 0.6661 - acc: 0.5928: 100%|██████████|00:00<00:00"
     ]
    }
   ],
   "source": [
    "workout.fit(data, epochs=5, callbacks=[meter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We now have a trained model and that is all for this notebook. But there are other notebooks available that dive into more details and there is also the API documentation itself. Please visit [github](https://github.com/neurallayer/fos) to find out more.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}